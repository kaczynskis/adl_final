# ADL Final Project
## About
This project is an extension of my capstone project, "Automatic Guitar Tablature Generation Using Deep Learning," which seeks to use image classification using a CNN to perform pitch recognition for the purpose of generating guitar tablature. The data consists of 12 sets of 22 notes (A2-G4) played on guitar, in .WAV format. The "preprocessing.py" code was used to generate spectrograms from each audio file and save them to sub-folders within the "images" folder, each named for the note it contains, in scientific pitch notation. Splitting data into training/testing sets and building/training/evaluating the model is done in a Google Colab notebook: https://colab.research.google.com/drive/11NcX9Q3xgQNAGUPjZrq3MhTzedZSfjTT?usp=sharing
## Running the Code
If you wish to add additional data, add a set of notes in a folder called "set[x]_wav" to the "notes" folder, then set the number of sets in "preprocessing.py" to x and run that code to generate additional spectrogram images. Run the cells in the Colab notebook to retrain the model with the additional data and evaluate its performance. Make sure that the files in this repository are uploaded to the session; you might want to first add them to your Google Drive and then mount your drive. If you wish to generate tablature for a note, run the cells for uploading a file then call the predict_note() function with the name of that file.
## Analysis
The changes made since completing my capstone project include adding a dropout layer to combat overfitting, adding an additional set of data, adding functionality to upload a wav file and generate tablature from it based on the model, and generating a confusion matrix to analyze the model's performance. With these changes, the training accuracy was slightly lower, the validation accuracy was comparable but consistently higher, and the testing accuracy was not as high as it had gotten when running the original model. However, the confusion matrix shows that most notes were correctly classified at least once using the testing data. ![image](https://user-images.githubusercontent.com/47861527/184788669-f9377196-e6c1-4966-bc6e-cd0111ad0fde.png)  
Looking at a diagram of notes on a guitar, we can see how significantly (or not significantly) some of these notes were misclassified. ![guitar scientific pitch notation](https://user-images.githubusercontent.com/47861527/184791413-08438673-4657-46f8-9ddd-cbbdc9a229aa.png)  
For example, an instance of C3 was misclassified as B2. Instances of each of those notes are next to each other on the fretboard. It is also only two frets away from an instance of A2, which it was also misclassified as. An interesting observation is that some notes were classified correctly and misclassified as other notes in their pitch class, namely E3. There are patterns which exist between frequencies of notes within a pitch class, so perhaps this is the cause for that phenomenon.


## Conclusion/Future Work
The changes made between my capstone project and this project did not make significant improvements to the model, which is still likely due to having a relatively small amount of data. However, the additional time for analysis helped give me a better overview of the model's performance, especially its ability to classify individual notes. Additionally, the tablature generation feature was implemented (in a basic form), demonstrating that there is potential for this as a useful tool once more improvements are made. Besides adding more data to better train the model, I would like to implement audio distortion of existing notes and generate spectrograms from them, and improve the tablature generation functionality such that it can generate tablature for more than one note at a time.
